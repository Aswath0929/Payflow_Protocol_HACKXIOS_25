#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     PAYFLOW QWEN3 LOCAL LLM SETUP                                     â•‘
â•‘                                                                                       â•‘
â•‘   Setup Script for Running Qwen3 on RTX 4070 (8GB VRAM)                              â•‘
â•‘                                                                                       â•‘
â•‘   This script:                                                                        â•‘
â•‘   1. Checks if Ollama is installed                                                   â•‘
â•‘   2. Starts Ollama server                                                            â•‘
â•‘   3. Pulls the Qwen3 model                                                           â•‘
â•‘   4. Tests the model                                                                 â•‘
â•‘                                                                                       â•‘
â•‘   Hackxios 2K25 - PayFlow Protocol                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import subprocess
import sys
import time
import os

# Qwen3 model for RTX 4070 8GB VRAM
# Options:
#   qwen3:1.7b  - ~1.5GB VRAM (fastest, basic analysis)
#   qwen3:4b    - ~3GB VRAM (good balance)
#   qwen3:8b    - ~5GB VRAM (recommended for RTX 4070)
QWEN3_MODEL = "qwen3:8b"

def print_banner():
    """Print the setup banner."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     PAYFLOW QWEN3 LOCAL LLM SETUP                                     â•‘
â•‘                                                                                       â•‘
â•‘   100% LOCAL AI - No Cloud API Keys Needed!                                          â•‘
â•‘   Running on your RTX 4070 GPU                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

def check_ollama_installed():
    """Check if Ollama is installed."""
    print("\n1. Checking Ollama installation...")
    try:
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode == 0:
            print(f"   âœ… Ollama is installed: {result.stdout.strip()}")
            return True
        else:
            print("   âŒ Ollama not found")
            return False
    except FileNotFoundError:
        print("   âŒ Ollama is not installed")
        return False
    except Exception as e:
        print(f"   âŒ Error checking Ollama: {e}")
        return False

def install_ollama_instructions():
    """Print Ollama installation instructions."""
    print("""
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   INSTALL OLLAMA
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   
   Windows:
   1. Download from: https://ollama.ai/download/windows
   2. Run the installer
   3. Restart this script
   
   Or use winget:
   > winget install Ollama.Ollama
   
   After installation, run this script again.
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

def check_ollama_server():
    """Check if Ollama server is running."""
    print("\n2. Checking Ollama server status...")
    try:
        import httpx
        response = httpx.get("http://localhost:11434/api/tags", timeout=5.0)
        if response.status_code == 200:
            print("   âœ… Ollama server is running")
            return True
        else:
            print("   âš ï¸ Ollama server responded with error")
            return False
    except:
        print("   âš ï¸ Ollama server is not running")
        return False

def start_ollama_server():
    """Start Ollama server in background."""
    print("\n   Starting Ollama server...")
    try:
        # On Windows, start Ollama in a new process
        if sys.platform == "win32":
            subprocess.Popen(
                ["ollama", "serve"],
                creationflags=subprocess.CREATE_NEW_CONSOLE,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
        else:
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True
            )
        
        # Wait for server to start
        print("   Waiting for server to start...")
        for i in range(10):
            time.sleep(1)
            try:
                import httpx
                response = httpx.get("http://localhost:11434/api/tags", timeout=2.0)
                if response.status_code == 200:
                    print("   âœ… Ollama server started successfully!")
                    return True
            except:
                print(f"   ... attempt {i+1}/10")
                continue
        
        print("   âŒ Failed to start Ollama server")
        return False
    except Exception as e:
        print(f"   âŒ Error starting server: {e}")
        return False

def check_qwen3_model():
    """Check if Qwen3 model is installed."""
    print(f"\n3. Checking for {QWEN3_MODEL} model...")
    try:
        import httpx
        response = httpx.get("http://localhost:11434/api/tags", timeout=5.0)
        if response.status_code == 200:
            data = response.json()
            models = [m["name"] for m in data.get("models", [])]
            
            # Check for exact match or variant
            for model in models:
                if "qwen3" in model.lower():
                    print(f"   âœ… Found Qwen3 model: {model}")
                    return True
            
            print(f"   âš ï¸ Qwen3 not found. Available models: {models}")
            return False
        return False
    except Exception as e:
        print(f"   âŒ Error checking models: {e}")
        return False

def pull_qwen3_model():
    """Pull the Qwen3 model."""
    print(f"\n   Pulling {QWEN3_MODEL}... (this may take 5-10 minutes)")
    print("   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    try:
        result = subprocess.run(
            ["ollama", "pull", QWEN3_MODEL],
            capture_output=False,
            text=True,
            timeout=1800  # 30 minute timeout
        )
        if result.returncode == 0:
            print(f"   âœ… {QWEN3_MODEL} downloaded successfully!")
            return True
        else:
            print(f"   âŒ Failed to pull {QWEN3_MODEL}")
            return False
    except subprocess.TimeoutExpired:
        print("   âŒ Download timed out (30 minutes)")
        return False
    except Exception as e:
        print(f"   âŒ Error pulling model: {e}")
        return False

def test_qwen3():
    """Test Qwen3 model with a simple prompt."""
    print("\n4. Testing Qwen3 model...")
    try:
        import httpx
        
        print("   Sending test prompt...")
        start = time.time()
        
        response = httpx.post(
            "http://localhost:11434/api/generate",
            json={
                "model": QWEN3_MODEL,
                "prompt": "Analyze this transaction for fraud: Amount $9,999 from new sender to offshore account. Is this suspicious?",
                "stream": False,
                "options": {
                    "num_predict": 100
                }
            },
            timeout=60.0
        )
        
        elapsed = time.time() - start
        
        if response.status_code == 200:
            result = response.json()
            text = result.get("response", "")[:200]
            print(f"   âœ… Qwen3 responded in {elapsed:.1f}s")
            print(f"\n   Response preview:")
            print(f"   {text}...")
            return True
        else:
            print(f"   âŒ Qwen3 API error: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ Error testing model: {e}")
        return False

def print_success():
    """Print success message."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                       â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â•‘
â•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â•‘
â•‘  â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ•â•   â•‘
â•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•—   â•‘
â•‘   â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•     â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•   â•‘
â•‘                                                                                       â•‘
â•‘   ğŸ‰ Qwen3 Local LLM is ready for fraud detection!                                    â•‘
â•‘                                                                                       â•‘
â•‘   Your Setup:                                                                         â•‘
â•‘   â€¢ Model: qwen3:8b (Latest 2025 release from Alibaba)                               â•‘
â•‘   â€¢ GPU: RTX 4070 (8GB VRAM)                                                         â•‘
â•‘   â€¢ Inference: 100% LOCAL - No cloud API, no data leaves your machine               â•‘
â•‘   â€¢ Speed: <500ms per analysis                                                       â•‘
â•‘                                                                                       â•‘
â•‘   To start the fraud detection server:                                               â•‘
â•‘   > cd theblocks/packages/nextjs/services/ai                                         â•‘
â•‘   > python -m uvicorn secureAIOracle:app --host 0.0.0.0 --port 8000                  â•‘
â•‘                                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

def main():
    """Main setup function."""
    print_banner()
    
    # Step 1: Check Ollama installed
    if not check_ollama_installed():
        install_ollama_instructions()
        return
    
    # Step 2: Check/start Ollama server
    if not check_ollama_server():
        if not start_ollama_server():
            print("\nâŒ Could not start Ollama server. Please run 'ollama serve' manually.")
            return
    
    # Step 3: Check/pull Qwen3 model
    if not check_qwen3_model():
        if not pull_qwen3_model():
            print(f"\nâŒ Could not pull {QWEN3_MODEL}. Please run 'ollama pull {QWEN3_MODEL}' manually.")
            return
    
    # Step 4: Test the model
    if not test_qwen3():
        print("\nâš ï¸ Qwen3 test failed but model is installed. Try restarting Ollama.")
        return
    
    # Success!
    print_success()

if __name__ == "__main__":
    # Check for httpx
    try:
        import httpx
    except ImportError:
        print("Installing httpx...")
        subprocess.run([sys.executable, "-m", "pip", "install", "httpx"], check=True)
        import httpx
    
    main()
